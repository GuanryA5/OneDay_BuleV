#!/usr/bin/env python3
"""
BlueV æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹è¯•å„ä¸ªç»„ä»¶çš„æ€§èƒ½æŒ‡æ ‡å¹¶å»ºç«‹åŸºå‡†
"""

import json
import subprocess
import sys
import time
from pathlib import Path
from typing import Dict

import psutil


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.results: Dict[str, Dict] = {}

    def measure_time_and_memory(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨"""
        process = psutil.Process()

        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB

        # æ‰§è¡Œå‡½æ•°
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            result = str(e)
            success = False

        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB

        return {
            "success": success,
            "result": result,
            "duration": end_time - start_time,
            "memory_start": start_memory,
            "memory_end": end_memory,
            "memory_delta": end_memory - start_memory,
        }

    def benchmark_config_loading(self) -> Dict:
        """åŸºå‡†æµ‹è¯•ï¼šé…ç½®åŠ è½½æ€§èƒ½"""
        print("ğŸ“Š åŸºå‡†æµ‹è¯•ï¼šé…ç½®åŠ è½½æ€§èƒ½")

        def load_config():
            from bluev.config import Config

            config = Config()
            return f"Loaded config for {config.APP_NAME}"

        # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
        results = []
        for _i in range(5):
            result = self.measure_time_and_memory(load_config)
            results.append(result)

        avg_duration = sum(r["duration"] for r in results) / len(results)
        avg_memory = sum(r["memory_delta"] for r in results) / len(results)

        benchmark = {
            "test_name": "Config Loading",
            "iterations": len(results),
            "avg_duration": avg_duration,
            "avg_memory_delta": avg_memory,
            "target_duration": 0.1,  # ç›®æ ‡ï¼š100ms å†…
            "target_memory": 5.0,  # ç›®æ ‡ï¼š5MB å†…
            "status": "PASS" if avg_duration < 0.1 and avg_memory < 5.0 else "FAIL",
        }

        print(f"  å¹³å‡è€—æ—¶: {avg_duration:.3f}s (ç›®æ ‡: <0.1s)")
        print(f"  å¹³å‡å†…å­˜: {avg_memory:.2f}MB (ç›®æ ‡: <5MB)")
        print(f"  çŠ¶æ€: {benchmark['status']}")

        return benchmark

    def benchmark_logging_performance(self) -> Dict:
        """åŸºå‡†æµ‹è¯•ï¼šæ—¥å¿—ç³»ç»Ÿæ€§èƒ½"""
        print("ğŸ“Š åŸºå‡†æµ‹è¯•ï¼šæ—¥å¿—ç³»ç»Ÿæ€§èƒ½")

        def logging_test():
            from bluev.utils.logging import get_logger

            logger = get_logger("benchmark")

            # å†™å…¥100æ¡æ—¥å¿—
            for i in range(100):
                logger.info(f"Benchmark log message {i}")

            return "100 log messages written"

        result = self.measure_time_and_memory(logging_test)

        benchmark = {
            "test_name": "Logging Performance",
            "iterations": 100,
            "duration": result["duration"],
            "memory_delta": result["memory_delta"],
            "target_duration": 1.0,  # ç›®æ ‡ï¼š1s å†…å†™å…¥100æ¡
            "target_memory": 10.0,  # ç›®æ ‡ï¼š10MB å†…
            "status": "PASS"
            if result["duration"] < 1.0 and result["memory_delta"] < 10.0
            else "FAIL",
        }

        print(f"  100æ¡æ—¥å¿—è€—æ—¶: {result['duration']:.3f}s (ç›®æ ‡: <1s)")
        print(f"  å†…å­˜å¢é•¿: {result['memory_delta']:.2f}MB (ç›®æ ‡: <10MB)")
        print(f"  çŠ¶æ€: {benchmark['status']}")

        return benchmark

    def benchmark_ruff_performance(self) -> Dict:
        """åŸºå‡†æµ‹è¯•ï¼šRuff æ€§èƒ½"""
        print("ğŸ“Š åŸºå‡†æµ‹è¯•ï¼šRuff ä»£ç æ£€æŸ¥æ€§èƒ½")

        python_exe = self.project_root / "venv" / "Scripts" / "python.exe"
        if not python_exe.exists():
            python_exe = self.project_root / "venv" / "bin" / "python"

        def run_ruff():
            # åªæ£€æŸ¥æ ¸å¿ƒæ¨¡å—ä»¥é¿å…è¶…æ—¶
            result = subprocess.run(
                [str(python_exe), "-m", "ruff", "check", "bluev/", "--quiet"],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=self.project_root,
            )
            return f"Ruff check completed with exit code {result.returncode}"

        result = self.measure_time_and_memory(run_ruff)

        benchmark = {
            "test_name": "Ruff Code Check",
            "duration": result["duration"],
            "memory_delta": result["memory_delta"],
            "target_duration": 30.0,  # ç›®æ ‡ï¼š30s å†…
            "target_memory": 50.0,  # ç›®æ ‡ï¼š50MB å†…
            "status": "PASS"
            if result["success"] and result["duration"] < 30.0
            else "FAIL",
        }

        print(f"  Ruff æ£€æŸ¥è€—æ—¶: {result['duration']:.3f}s (ç›®æ ‡: <30s)")
        print(f"  å†…å­˜ä½¿ç”¨: {result['memory_delta']:.2f}MB (ç›®æ ‡: <50MB)")
        print(f"  çŠ¶æ€: {benchmark['status']}")

        return benchmark

    def benchmark_import_performance(self) -> Dict:
        """åŸºå‡†æµ‹è¯•ï¼šæ¨¡å—å¯¼å…¥æ€§èƒ½"""
        print("ğŸ“Š åŸºå‡†æµ‹è¯•ï¼šæ¨¡å—å¯¼å…¥æ€§èƒ½")

        def import_modules():
            # æµ‹è¯•ä¸»è¦æ¨¡å—çš„å¯¼å…¥æ—¶é—´
            return "Core modules imported"

        result = self.measure_time_and_memory(import_modules)

        benchmark = {
            "test_name": "Module Import",
            "duration": result["duration"],
            "memory_delta": result["memory_delta"],
            "target_duration": 2.0,  # ç›®æ ‡ï¼š2s å†…
            "target_memory": 20.0,  # ç›®æ ‡ï¼š20MB å†…
            "status": "PASS"
            if result["success"] and result["duration"] < 2.0
            else "FAIL",
        }

        print(f"  æ¨¡å—å¯¼å…¥è€—æ—¶: {result['duration']:.3f}s (ç›®æ ‡: <2s)")
        print(f"  å†…å­˜ä½¿ç”¨: {result['memory_delta']:.2f}MB (ç›®æ ‡: <20MB)")
        print(f"  çŠ¶æ€: {benchmark['status']}")

        return benchmark

    def benchmark_application_startup(self) -> Dict:
        """åŸºå‡†æµ‹è¯•ï¼šåº”ç”¨ç¨‹åºå¯åŠ¨æ€§èƒ½"""
        print("ğŸ“Š åŸºå‡†æµ‹è¯•ï¼šåº”ç”¨ç¨‹åºå¯åŠ¨æ€§èƒ½")

        python_exe = self.project_root / "venv" / "Scripts" / "python.exe"
        if not python_exe.exists():
            python_exe = self.project_root / "venv" / "bin" / "python"

        def startup_test():
            # æµ‹è¯•åº”ç”¨ç¨‹åºåˆå§‹åŒ–ï¼ˆä¸å¯åŠ¨GUIï¼‰
            result = subprocess.run(
                [
                    str(python_exe),
                    "-c",
                    "from bluev.main import BlueVApplication; app = BlueVApplication(); print('App initialized')",
                ],
                capture_output=True,
                text=True,
                timeout=10,
                cwd=self.project_root,
            )
            return f"App startup test: {result.returncode}"

        result = self.measure_time_and_memory(startup_test)

        benchmark = {
            "test_name": "Application Startup",
            "duration": result["duration"],
            "memory_delta": result["memory_delta"],
            "target_duration": 5.0,  # ç›®æ ‡ï¼š5s å†…
            "target_memory": 30.0,  # ç›®æ ‡ï¼š30MB å†…
            "status": "PASS"
            if result["success"] and result["duration"] < 5.0
            else "FAIL",
        }

        print(f"  åº”ç”¨å¯åŠ¨è€—æ—¶: {result['duration']:.3f}s (ç›®æ ‡: <5s)")
        print(f"  å†…å­˜ä½¿ç”¨: {result['memory_delta']:.2f}MB (ç›®æ ‡: <30MB)")
        print(f"  çŠ¶æ€: {benchmark['status']}")

        return benchmark

    def run_all_benchmarks(self) -> Dict[str, Dict]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...\n")

        benchmarks = [
            self.benchmark_config_loading,
            self.benchmark_logging_performance,
            self.benchmark_import_performance,
            self.benchmark_ruff_performance,
            self.benchmark_application_startup,
        ]

        results = {}
        for benchmark_func in benchmarks:
            try:
                result = benchmark_func()
                results[result["test_name"]] = result
            except Exception as e:
                print(f"  âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
                results[benchmark_func.__name__] = {
                    "test_name": benchmark_func.__name__,
                    "status": "ERROR",
                    "error": str(e),
                }
            print()

        return results

    def save_benchmark_results(self, results: Dict):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        results_file = self.project_root / "benchmark_results.json"

        # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
        system_info = {
            "timestamp": time.time(),
            "python_version": sys.version,
            "platform": sys.platform,
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
        }

        full_results = {"system_info": system_info, "benchmarks": results}

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def print_summary(self, results: Dict):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ€»ç»“"""
        print("=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ€»ç»“")
        print("=" * 60)

        passed = sum(1 for r in results.values() if r.get("status") == "PASS")
        failed = sum(1 for r in results.values() if r.get("status") == "FAIL")
        errors = sum(1 for r in results.values() if r.get("status") == "ERROR")
        total = len(results)

        print(f"æ€»æµ‹è¯•æ•°: {total}")
        print(f"é€šè¿‡: {passed}")
        print(f"å¤±è´¥: {failed}")
        print(f"é”™è¯¯: {errors}")
        print(f"æˆåŠŸç‡: {passed/total*100:.1f}%")
        print()

        print("æ€§èƒ½æŒ‡æ ‡:")
        for test_name, result in results.items():
            if result.get("status") in ["PASS", "FAIL"]:
                duration = result.get("duration", result.get("avg_duration", 0))
                memory = result.get("memory_delta", result.get("avg_memory_delta", 0))
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"
                print(f"  {status_icon} {test_name}: {duration:.3f}s, {memory:.2f}MB")

        return passed == total


def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    results = benchmark.run_all_benchmarks()
    success = benchmark.print_summary(results)
    benchmark.save_benchmark_results(results)

    if success:
        print("\nğŸ‰ æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ€§èƒ½åŸºå‡†æµ‹è¯•æœªè¾¾æ ‡ï¼Œéœ€è¦ä¼˜åŒ–ã€‚")

    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
