# BlueVå·¥ä½œæµæ‰§è¡Œå¼•æ“è®¾è®¡

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-01-27
**å…³è”æ–‡æ¡£**: BlueV_Architecture_Design.md, Node_System_Implementation.md

---

## ğŸ¯ **æ‰§è¡Œå¼•æ“è®¾è®¡ç›®æ ‡**

### **æ ¸å¿ƒç‰¹æ€§**
- âš¡ **é«˜æ€§èƒ½**: å¼‚æ­¥æ‰§è¡Œå’Œå¹¶è¡Œå¤„ç†
- ğŸ”„ **çŠ¶æ€ç®¡ç†**: å®Œæ•´çš„æ‰§è¡ŒçŠ¶æ€è¿½è¸ª
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**: å¥å£®çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶
- ğŸ“Š **å¯è§‚æµ‹æ€§**: è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—å’Œç›‘æ§
- ğŸ”€ **çµæ´»è°ƒåº¦**: æ”¯æŒå¤šç§æ‰§è¡Œç­–ç•¥

---

## ğŸ—ï¸ **æ‰§è¡Œå¼•æ“æ ¸å¿ƒç»„ä»¶**

### **æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†**
```python
import asyncio
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class ExecutionMetrics:
    """æ‰§è¡ŒæŒ‡æ ‡"""
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    nodes_executed: int = 0
    nodes_failed: int = 0
    memory_usage: float = 0.0

class ExecutionContext:
    """å¢å¼ºçš„æ‰§è¡Œä¸Šä¸‹æ–‡"""

    def __init__(self, workflow_id: str):
        self.workflow_id = workflow_id
        self.variables: Dict[str, Any] = {}
        self.node_outputs: Dict[str, Dict[str, Any]] = {}
        self.execution_history: List[str] = []
        self.metrics = ExecutionMetrics()
        self.callbacks: Dict[str, List[Callable]] = {
            "node_start": [],
            "node_complete": [],
            "node_error": [],
            "workflow_complete": [],
            "workflow_error": []
        }

    def add_callback(self, event: str, callback: Callable):
        """æ·»åŠ äº‹ä»¶å›è°ƒ"""
        if event in self.callbacks:
            self.callbacks[event].append(callback)

    async def trigger_callback(self, event: str, **kwargs):
        """è§¦å‘äº‹ä»¶å›è°ƒ"""
        for callback in self.callbacks.get(event, []):
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(self, **kwargs)
                else:
                    callback(self, **kwargs)
            except Exception as e:
                print(f"Callback error for {event}: {e}")

    def get_variable(self, name: str, default: Any = None) -> Any:
        return self.variables.get(name, default)

    def set_variable(self, name: str, value: Any):
        self.variables[name] = value

    def get_node_output(self, node_id: str, output_name: str, default: Any = None) -> Any:
        return self.node_outputs.get(node_id, {}).get(output_name, default)

    def set_node_output(self, node_id: str, outputs: Dict[str, Any]):
        self.node_outputs[node_id] = outputs

    def get_execution_summary(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        return {
            "workflow_id": self.workflow_id,
            "metrics": {
                "start_time": self.metrics.start_time.isoformat(),
                "end_time": self.metrics.end_time.isoformat() if self.metrics.end_time else None,
                "duration": self.metrics.duration,
                "nodes_executed": self.metrics.nodes_executed,
                "nodes_failed": self.metrics.nodes_failed,
                "memory_usage": self.metrics.memory_usage
            },
            "execution_history": self.execution_history,
            "variables": self.variables
        }
```

### **å·¥ä½œæµæ‰§è¡Œå¼•æ“**
```python
import asyncio
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import logging

class WorkflowExecutionError(Exception):
    """å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸"""
    def __init__(self, message: str, node_id: str = None, original_error: Exception = None):
        super().__init__(message)
        self.node_id = node_id
        self.original_error = original_error

class WorkflowEngine:
    """BlueVå·¥ä½œæµæ‰§è¡Œå¼•æ“"""

    def __init__(self, max_concurrent_nodes: int = 5):
        self.nodes: Dict[str, BaseNode] = {}
        self.connections: List[Tuple[str, str, str, str]] = []
        self.max_concurrent_nodes = max_concurrent_nodes
        self.logger = logging.getLogger(__name__)
        self._execution_semaphore = asyncio.Semaphore(max_concurrent_nodes)

    def add_node(self, node: BaseNode):
        """æ·»åŠ èŠ‚ç‚¹"""
        self.nodes[node.node_id] = node
        self.logger.info(f"Added node: {node.node_id} ({node.__class__.__name__})")

    def remove_node(self, node_id: str):
        """ç§»é™¤èŠ‚ç‚¹"""
        if node_id in self.nodes:
            del self.nodes[node_id]
            # ç§»é™¤ç›¸å…³è¿æ¥
            self.connections = [
                conn for conn in self.connections
                if conn[0] != node_id and conn[2] != node_id
            ]
            self.logger.info(f"Removed node: {node_id}")

    def add_connection(self, from_node: str, from_output: str, to_node: str, to_input: str):
        """æ·»åŠ è¿æ¥"""
        if from_node not in self.nodes:
            raise ValueError(f"Source node '{from_node}' not found")
        if to_node not in self.nodes:
            raise ValueError(f"Target node '{to_node}' not found")

        self.connections.append((from_node, from_output, to_node, to_input))
        self.logger.info(f"Added connection: {from_node}.{from_output} -> {to_node}.{to_input}")

    def build_execution_graph(self) -> Tuple[Dict[str, Set[str]], Dict[str, int]]:
        """æ„å»ºæ‰§è¡Œå›¾"""
        graph = defaultdict(set)
        in_degree = defaultdict(int)

        # åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹
        for node_id in self.nodes:
            in_degree[node_id] = 0

        # æ„å»ºä¾èµ–å…³ç³»
        for from_node, _, to_node, _ in self.connections:
            if to_node not in graph[from_node]:
                graph[from_node].add(to_node)
                in_degree[to_node] += 1

        return graph, in_degree

    def topological_sort(self) -> List[str]:
        """æ‹“æ‰‘æ’åºç¡®å®šæ‰§è¡Œé¡ºåº"""
        graph, in_degree = self.build_execution_graph()
        queue = deque([node for node, degree in in_degree.items() if degree == 0])
        result = []

        while queue:
            current = queue.popleft()
            result.append(current)

            for neighbor in graph[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        if len(result) != len(self.nodes):
            cycle_nodes = [node for node, degree in in_degree.items() if degree > 0]
            raise WorkflowExecutionError(f"Workflow contains cycles involving nodes: {cycle_nodes}")

        return result

    async def execute_workflow(self, context: ExecutionContext) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµ"""
        self.logger.info(f"Starting workflow execution: {context.workflow_id}")
        context.metrics.start_time = datetime.now()

        try:
            # è·å–æ‰§è¡Œé¡ºåº
            execution_order = self.topological_sort()
            self.logger.info(f"Execution order: {execution_order}")

            # è§¦å‘å·¥ä½œæµå¼€å§‹å›è°ƒ
            await context.trigger_callback("workflow_start", execution_order=execution_order)

            results = {}

            # é¡ºåºæ‰§è¡ŒèŠ‚ç‚¹
            for node_id in execution_order:
                node = self.nodes[node_id]

                try:
                    # æ‰§è¡ŒèŠ‚ç‚¹
                    node_result = await self._execute_node(node, context)
                    results[node_id] = node_result
                    context.metrics.nodes_executed += 1

                except Exception as e:
                    context.metrics.nodes_failed += 1
                    self.logger.error(f"Node {node_id} execution failed: {e}")

                    # è§¦å‘èŠ‚ç‚¹é”™è¯¯å›è°ƒ
                    await context.trigger_callback("node_error", node_id=node_id, error=e)

                    # æ ¹æ®é”™è¯¯å¤„ç†ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­
                    if self._should_stop_on_error(node, e):
                        raise WorkflowExecutionError(
                            f"Workflow stopped due to node {node_id} failure: {e}",
                            node_id=node_id,
                            original_error=e
                        )

            # è®¡ç®—æ‰§è¡ŒæŒ‡æ ‡
            context.metrics.end_time = datetime.now()
            context.metrics.duration = (
                context.metrics.end_time - context.metrics.start_time
            ).total_seconds()

            # è§¦å‘å·¥ä½œæµå®Œæˆå›è°ƒ
            await context.trigger_callback("workflow_complete", results=results)

            self.logger.info(f"Workflow execution completed: {context.workflow_id}")
            return results

        except Exception as e:
            context.metrics.end_time = datetime.now()
            context.metrics.duration = (
                context.metrics.end_time - context.metrics.start_time
            ).total_seconds()

            # è§¦å‘å·¥ä½œæµé”™è¯¯å›è°ƒ
            await context.trigger_callback("workflow_error", error=e)

            self.logger.error(f"Workflow execution failed: {context.workflow_id}, error: {e}")
            raise

    async def _execute_node(self, node: BaseNode, context: ExecutionContext) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        async with self._execution_semaphore:
            self.logger.info(f"Executing node: {node.node_id}")

            # è§¦å‘èŠ‚ç‚¹å¼€å§‹å›è°ƒ
            await context.trigger_callback("node_start", node_id=node.node_id)

            try:
                # å‡†å¤‡èŠ‚ç‚¹è¾“å…¥
                await self._prepare_node_inputs(node, context)

                # éªŒè¯è¾“å…¥
                node.validate_inputs()

                # æ‰§è¡ŒèŠ‚ç‚¹
                node.state = NodeState.RUNNING
                outputs = await node.execute(context)

                # ä¿å­˜è¾“å‡º
                context.set_node_output(node.node_id, outputs)
                context.execution_history.append(node.node_id)

                # è§¦å‘èŠ‚ç‚¹å®Œæˆå›è°ƒ
                await context.trigger_callback("node_complete", node_id=node.node_id, outputs=outputs)

                self.logger.info(f"Node {node.node_id} completed successfully")
                return outputs

            except Exception as e:
                node.state = NodeState.FAILED
                node.error_message = str(e)
                self.logger.error(f"Node {node.node_id} failed: {e}")
                raise

    async def _prepare_node_inputs(self, node: BaseNode, context: ExecutionContext):
        """å‡†å¤‡èŠ‚ç‚¹è¾“å…¥æ•°æ®"""
        for from_node, from_output, to_node, to_input in self.connections:
            if to_node == node.node_id:
                output_value = context.get_node_output(from_node, from_output)
                if output_value is not None:
                    node.inputs[to_input] = output_value
                    self.logger.debug(f"Set input {to_input} for node {node.node_id}")

    def _should_stop_on_error(self, node: BaseNode, error: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å› é”™è¯¯åœæ­¢æ‰§è¡Œ"""
        # å¯ä»¥æ ¹æ®èŠ‚ç‚¹ç±»å‹æˆ–é”™è¯¯ç±»å‹åˆ¶å®šä¸åŒçš„ç­–ç•¥
        # è¿™é‡Œç®€å•åœ°å¯¹æ‰€æœ‰é”™è¯¯éƒ½åœæ­¢æ‰§è¡Œ
        return True

    def get_workflow_status(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        node_states = {}
        for node_id, node in self.nodes.items():
            node_states[node_id] = {
                "state": node.state.value,
                "error_message": node.error_message
            }

        return {
            "nodes": node_states,
            "connections": self.connections,
            "total_nodes": len(self.nodes),
            "total_connections": len(self.connections)
        }

    def validate_workflow(self) -> List[str]:
        """éªŒè¯å·¥ä½œæµå®Œæ•´æ€§"""
        errors = []

        # æ£€æŸ¥å¾ªç¯ä¾èµ–
        try:
            self.topological_sort()
        except WorkflowExecutionError as e:
            errors.append(str(e))

        # æ£€æŸ¥è¿æ¥æœ‰æ•ˆæ€§
        for from_node, from_output, to_node, to_input in self.connections:
            if from_node not in self.nodes:
                errors.append(f"Connection references non-existent source node: {from_node}")
            if to_node not in self.nodes:
                errors.append(f"Connection references non-existent target node: {to_node}")

        # æ£€æŸ¥èŠ‚ç‚¹è¾“å…¥å®Œæ•´æ€§
        for node_id, node in self.nodes.items():
            input_spec = node.get_input_spec()
            required_inputs = {spec.name for spec in input_spec if spec.required}

            # æ£€æŸ¥å“ªäº›è¾“å…¥æœ‰è¿æ¥
            connected_inputs = set()
            for _, _, to_node, to_input in self.connections:
                if to_node == node_id:
                    connected_inputs.add(to_input)

            # æ£€æŸ¥ç¼ºå¤±çš„å¿…éœ€è¾“å…¥
            missing_inputs = required_inputs - connected_inputs
            for missing_input in missing_inputs:
                # æ£€æŸ¥æ˜¯å¦æœ‰é»˜è®¤å€¼
                spec = next((s for s in input_spec if s.name == missing_input), None)
                if spec and spec.default_value is None:
                    errors.append(f"Node {node_id} missing required input: {missing_input}")

        return errors
```

---

## ğŸ”„ **å¹¶è¡Œæ‰§è¡Œæ”¯æŒ**

### **å¹¶è¡Œæ‰§è¡Œå¼•æ“**
```python
class ParallelWorkflowEngine(WorkflowEngine):
    """æ”¯æŒå¹¶è¡Œæ‰§è¡Œçš„å·¥ä½œæµå¼•æ“"""

    def __init__(self, max_concurrent_nodes: int = 5, executor_type: str = "thread"):
        super().__init__(max_concurrent_nodes)
        self.executor_type = executor_type
        self.executor = None

    async def execute_workflow_parallel(self, context: ExecutionContext) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œå·¥ä½œæµ"""
        self.logger.info(f"Starting parallel workflow execution: {context.workflow_id}")

        # æ„å»ºæ‰§è¡Œå›¾
        graph, in_degree = self.build_execution_graph()

        # åˆå§‹åŒ–æ‰§è¡ŒçŠ¶æ€
        completed_nodes = set()
        running_tasks = {}
        results = {}

        # åˆ›å»ºæ‰§è¡Œå™¨
        if self.executor_type == "thread":
            self.executor = ThreadPoolExecutor(max_workers=self.max_concurrent_nodes)
        elif self.executor_type == "process":
            self.executor = ProcessPoolExecutor(max_workers=self.max_concurrent_nodes)

        try:
            context.metrics.start_time = datetime.now()

            while len(completed_nodes) < len(self.nodes):
                # æ‰¾åˆ°å¯ä»¥æ‰§è¡Œçš„èŠ‚ç‚¹
                ready_nodes = [
                    node_id for node_id in self.nodes
                    if node_id not in completed_nodes
                    and node_id not in running_tasks
                    and in_degree[node_id] == 0
                ]

                # å¯åŠ¨å°±ç»ªèŠ‚ç‚¹çš„æ‰§è¡Œ
                for node_id in ready_nodes:
                    if len(running_tasks) < self.max_concurrent_nodes:
                        node = self.nodes[node_id]
                        task = asyncio.create_task(self._execute_node(node, context))
                        running_tasks[node_id] = task

                # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                if running_tasks:
                    done, pending = await asyncio.wait(
                        running_tasks.values(),
                        return_when=asyncio.FIRST_COMPLETED
                    )

                    # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                    for task in done:
                        # æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹ID
                        node_id = next(
                            nid for nid, t in running_tasks.items() if t == task
                        )

                        try:
                            result = await task
                            results[node_id] = result
                            completed_nodes.add(node_id)
                            context.metrics.nodes_executed += 1

                            # æ›´æ–°ä¾èµ–èŠ‚ç‚¹çš„å…¥åº¦
                            for dependent in graph[node_id]:
                                in_degree[dependent] -= 1

                        except Exception as e:
                            context.metrics.nodes_failed += 1
                            self.logger.error(f"Parallel node {node_id} failed: {e}")

                            # æ ¹æ®é”™è¯¯å¤„ç†ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­
                            if self._should_stop_on_error(self.nodes[node_id], e):
                                # å–æ¶ˆæ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
                                for pending_task in pending:
                                    pending_task.cancel()
                                raise WorkflowExecutionError(
                                    f"Parallel workflow stopped due to node {node_id} failure: {e}",
                                    node_id=node_id,
                                    original_error=e
                                )

                        # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
                        del running_tasks[node_id]

            # è®¡ç®—æ‰§è¡ŒæŒ‡æ ‡
            context.metrics.end_time = datetime.now()
            context.metrics.duration = (
                context.metrics.end_time - context.metrics.start_time
            ).total_seconds()

            self.logger.info(f"Parallel workflow execution completed: {context.workflow_id}")
            return results

        finally:
            if self.executor:
                self.executor.shutdown(wait=True)
```

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å·¥ä½œæµæ‰§è¡Œå¼•æ“è®¾è®¡å·²å®Œæˆ
**æ ¸å¿ƒç‰¹æ€§**: é«˜æ€§èƒ½ã€çŠ¶æ€ç®¡ç†ã€é”™è¯¯å¤„ç†ã€å¯è§‚æµ‹æ€§ã€çµæ´»è°ƒåº¦
**æ‰§è¡Œæ¨¡å¼**: é¡ºåºæ‰§è¡Œ + å¹¶è¡Œæ‰§è¡Œæ”¯æŒ
**ç›‘æ§èƒ½åŠ›**: å®Œæ•´çš„æ‰§è¡ŒæŒ‡æ ‡å’Œå›è°ƒæœºåˆ¶
**ä¸‹ä¸€æ­¥**: å®ç°å®æ—¶é€šä¿¡ç³»ç»Ÿå’ŒPySide6é›†æˆ
